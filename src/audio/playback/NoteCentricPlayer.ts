/**
 * NoteCentricPlayer
 *
 * Plays musical phrases generated by NoteCentricMapper.
 * Handles timing, polyphony, and instrument selection for
 * rich center-note-focused playback.
 */

import type { MusicalPhrase, NoteCentricMapping } from '../mapping/NoteCentricMapper';
import type { AudioEngine } from '../audio-engine';
import { getLogger } from '../../logging';

const logger = getLogger('NoteCentricPlayer');

interface PlayingNote {
	instrumentName: string;
	frequency: number;
	stopTime: number;
}

/**
 * NoteCentricPlayer class
 */
export class NoteCentricPlayer {
	private audioEngine: AudioEngine;
	private isPlaying: boolean = false;
	private playingNotes: PlayingNote[] = [];
	private animationFrameId: number | null = null;
	private startTime: number = 0;
	private embellishmentCounts: Record<string, number> = {};

	constructor(audioEngine: AudioEngine) {
		this.audioEngine = audioEngine;
	}

	/**
	 * Play a note-centric mapping
	 */
	public async play(mapping: NoteCentricMapping): Promise<void> {
		if (this.isPlaying) {
			logger.warn('already-playing', 'Already playing, stopping first');
			this.stop();
		}

		logger.info('playback-start', 'Starting note-centric playback', {
			centerPhraseLength: mapping.centerPhrase.melody.length,
			embellishments: mapping.embellishments.length,
			tempo: mapping.centerPhrase.tempo,
			audioEngineStatus: this.audioEngine.getStatus()
		});

		this.isPlaying = true;
		this.startTime = Date.now();

		// Reset embellishment counts for staggering
		this.embellishmentCounts = {
			'harmonic-response': 0,
			'rhythmic-counterpoint': 0,
			'ambient-texture': 0
		};

		// Play center phrase with lead instrument
		logger.debug('play-center', 'Playing center phrase');
		await this.playPhrase(mapping.centerPhrase, 'center', 0);

		// Play embellishments with appropriate instruments
		logger.debug('play-embellishments', 'Playing embellishments', {
			count: mapping.embellishments.length
		});
		for (const embellishment of mapping.embellishments) {
			const delay = this.getDelayForEmbellishment(embellishment.type);
			await this.playPhrase(embellishment.phrase, embellishment.type, delay);
		}

		// Start animation loop to clean up finished notes
		logger.debug('start-animation', 'Starting animation loop');
		this.startAnimationLoop();

		logger.info('playback-scheduled', 'All notes scheduled', {
			totalNotes: this.playingNotes.length
		});
	}

	/**
	 * Stop playback
	 */
	public stop(): void {
		if (!this.isPlaying) {
			return;
		}

		logger.info('playback-stop', 'Stopping note-centric playback');

		// Notes stop automatically after their duration in AudioEngine
		// Just clear tracking and stop scheduling new notes
		this.playingNotes = [];
		this.isPlaying = false;

		if (this.animationFrameId !== null) {
			cancelAnimationFrame(this.animationFrameId);
			this.animationFrameId = null;
		}
	}

	/**
	 * Check if currently playing
	 */
	public getIsPlaying(): boolean {
		return this.isPlaying;
	}

	/**
	 * Get count of currently active voices
	 */
	public getActiveVoiceCount(): number {
		return this.playingNotes.length;
	}

	/**
	 * Play a musical phrase
	 */
	private async playPhrase(
		phrase: MusicalPhrase,
		role: string,
		startDelay: number
	): Promise<void> {
		// Get instrument once per phrase, not per note
		const instrument = this.getInstrumentForRole(role);

		logger.debug('phrase-play', 'Playing phrase', {
			role,
			instrument,
			length: phrase.melody.length,
			tempo: phrase.tempo,
			startDelay
		});

		// Calculate beat duration in milliseconds
		const beatDuration = (60 / phrase.tempo) * 1000;

		let currentTime = startDelay;

		// Pre-bind this reference for setTimeout callbacks
		const isPlayingRef = () => this.isPlaying;

		// Schedule all notes in the phrase
		for (let i = 0; i < phrase.melody.length; i++) {
			if (!this.isPlaying) {
				break;
			}

			const pitchOffset = phrase.melody[i];
			const chordRoot = phrase.harmony[i];
			const duration = phrase.rhythm[i];
			const velocity = phrase.velocities[i];

			// Validate all values before frequency calculation
			if (isNaN(pitchOffset) || isNaN(chordRoot) || isNaN(duration) || isNaN(velocity)) {
				logger.error('invalid-note-data', 'NaN detected in phrase data', {
					index: i,
					pitchOffset,
					chordRoot,
					duration,
					velocity,
					role,
					melodyLength: phrase.melody.length,
					harmonyLength: phrase.harmony.length,
					rhythmLength: phrase.rhythm.length,
					velocitiesLength: phrase.velocities.length
				});
				continue; // Skip this note
			}

			// Calculate frequency from pitch offset and chord root
			// Use C4 (261.63 Hz) as base
			const baseFreq = 261.63;
			const totalOffset = chordRoot + pitchOffset;
			const frequency = baseFreq * Math.pow(2, totalOffset / 12);

			// Add EXTREME micro-timing humanization (±100-150ms random offset)
			// This creates VERY noticeable "falling off the piano bench" groove looseness
			const humanization = (Math.random() * 150 - 75) + (Math.random() * 150 - 75); // ±100-150ms
			const humanizedTime = currentTime + humanization;

			logger.debug('schedule-note', 'Scheduling note', {
				index: i,
				delay: humanizedTime,
				humanization: humanization.toFixed(2),
				frequency: frequency.toFixed(2),
				role,
				instrument
			});

			// Schedule note to start with humanized timing
			setTimeout(async () => {
				logger.debug('timeout-fired', 'Note timeout fired', {
					index: i,
					isPlaying: this.isPlaying
				});
				if (!this.isPlaying) {
					return;
				}

				try {
					// Apply scale quantization if enabled
					let finalFreq = frequency;
					if (this.audioEngine.musicalTheoryEngine) {
						finalFreq = this.audioEngine.musicalTheoryEngine.constrainPitchToScale(frequency);
					}

					// Calculate note duration in seconds
					const durationSeconds = (duration * beatDuration) / 1000;

					// Play note using AudioEngine's playNoteImmediate
					await this.audioEngine.playNoteImmediate({
						pitch: finalFreq,
						duration: durationSeconds,
						velocity: velocity,
						instrument: instrument
					});

					// Track playing note
					const stopTime = Date.now() + duration * beatDuration;
					this.playingNotes.push({
						instrumentName: instrument,
						frequency: finalFreq,
						stopTime
					});

					logger.debug('note-play', 'Note started', {
						instrument,
						frequency: finalFreq.toFixed(2),
						velocity: velocity.toFixed(2),
						duration: durationSeconds.toFixed(2)
					});
				} catch (error) {
					logger.warn('note-play-error', 'Error playing note', {
						instrument,
						frequency: frequency.toFixed(2),
						error: error instanceof Error ? error.message : String(error),
						errorStack: error instanceof Error ? error.stack : undefined
					});
				}
			}, Math.max(0, humanizedTime)); // Use humanized time, ensure non-negative

			currentTime += duration * beatDuration;
		}

		// Note: Don't schedule automatic stop here - the animation loop
		// will handle cleanup when all notes finish naturally
	}

	/**
	 * Get instrument for a role
	 */
	private getInstrumentForRole(role: string): string {
		// Get list of enabled instruments from audio engine
		const enabledInstruments = this.audioEngine.getEnabledInstruments();

		if (enabledInstruments.length === 0) {
			logger.warn('no-instruments', 'No instruments enabled, cannot play note-centric audio');
			return 'piano'; // Fallback (will fail gracefully)
		}

		// Preferred instruments by role (in order of preference)
		const preferredInstruments: Record<string, string[]> = {
			'center': ['piano', 'organ', 'electricpiano', 'harpsichord', 'marimba'],
			'harmonic-response': ['electricpiano', 'organ', 'vibraphone', 'harp', 'celeste'],
			'rhythmic-counterpoint': ['cello', 'bassElectric', 'bassoon', 'trombone', 'tuba'],
			'ambient-texture': ['pad', 'organ', 'frenchHorn', 'clarinet', 'flute']
		};

		const preferences = preferredInstruments[role] || preferredInstruments['center'];

		// Find first preferred instrument that's enabled
		for (const preferred of preferences) {
			if (enabledInstruments.includes(preferred)) {
				return preferred;
			}
		}

		// If no preferred instrument is enabled, use first available
		logger.debug('instrument-fallback', 'Using fallback instrument for role', {
			role,
			instrument: enabledInstruments[0]
		});
		return enabledInstruments[0];
	}

	/**
	 * Get delay for embellishment type with phrase-sensitive timing and overlap
	 */
	private getDelayForEmbellishment(type: string): number {
		// Create temporal overlap for richer polyphonic texture
		// Embellishments now start EARLIER to overlap with center phrase
		if (!this.embellishmentCounts) {
			this.embellishmentCounts = {
				'harmonic-response': 0,
				'rhythmic-counterpoint': 0,
				'ambient-texture': 0
			};
		}

		// VERY EARLY base delays for MAXIMUM overlap and polyphonic density
		const baseDelays: Record<string, number> = {
			'harmonic-response': 1000 + (Math.random() * 1000),      // 1-2s (IMMEDIATE overlap!)
			'rhythmic-counterpoint': 2500 + (Math.random() * 1500),  // 2.5-4s (much earlier)
			'ambient-texture': 0 + (Math.random() * 300)             // 0-0.3s (essentially immediate)
		};

		// VERY SMALL stagger delays for DENSE polyphonic texture
		const staggerDelay: Record<string, number> = {
			'harmonic-response': 1500 + (Math.random() * 1000),      // 1.5-2.5s more each time
			'rhythmic-counterpoint': 2000 + (Math.random() * 1000),  // 2-3s more each time
			'ambient-texture': 1000 + (Math.random() * 500)          // 1-1.5s for ambient
		};

		const baseDelay = baseDelays[type] || 0;
		const count = this.embellishmentCounts[type] || 0;
		const stagger = (staggerDelay[type] || 0) * count;

		this.embellishmentCounts[type] = count + 1;

		// Round to nearest 100ms for cleaner timing
		return Math.round((baseDelay + stagger) / 100) * 100;
	}

	/**
	 * Animation loop to clean up finished notes
	 */
	private startAnimationLoop(): void {
		const loop = () => {
			if (!this.isPlaying) {
				return;
			}

			const now = Date.now();

			// Remove notes that have finished (they stop automatically in AudioEngine)
			this.playingNotes = this.playingNotes.filter(note => {
				return now < note.stopTime;
			});

			// If all notes have finished, stop playback
			// Wait a bit after scheduling completes to ensure no more notes incoming
			const timeSinceStart = now - this.startTime;
			if (this.playingNotes.length === 0 && timeSinceStart > 1000) {
				logger.info('playback-complete', 'All notes finished, stopping playback naturally');
				this.isPlaying = false;
				if (this.animationFrameId !== null) {
					cancelAnimationFrame(this.animationFrameId);
					this.animationFrameId = null;
				}
				return;
			}

			// Continue loop
			this.animationFrameId = requestAnimationFrame(loop);
		};

		this.animationFrameId = requestAnimationFrame(loop);
	}
}
